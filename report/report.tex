\documentclass[czech]{article}

\usepackage[utf8]{inputenc}
\usepackage[IL2]{fontenc}
\usepackage[czech]{babel}
\usepackage[a4paper,textheight=674pt]{geometry}
\usepackage{hyperref}
\usepackage{graphicx}

\begin{document}
\begin{center}\large
\bf Semestrální projekt MI-PDP 2017/2018:\\[6mm]
    Paralelní algoritmus pro řešení problému\\[3mm]
    Bílá královna na šachovnici -- KAS\\[6mm]
    Josef Doležal\\[2mm]
    magisterské studium, FIT ČVUT, Thákurova 9, 160 00 Praha 6\\[2mm]
    \today
\end{center}

\thispagestyle{empty}
\newpage

\section{Definice problému}

Problém bílé královny na šachovnici je nalezení minimálního počtu tahů, které musí královna udělat, aby vzala všechny oponentovi pěšce.
Tento problém je zadán jako šachovnice (o velikosti $n$), kolekce souřadnic pěšců a pozice královny.
Má-li úloha řešení, pak horní mez pro sebrání pěšců je rovna $3 \cdot q$, kde $q$ je počet pěšců.
Tato horní mez je mnohdy výrazně vyšší než skutečné řešení, z tohoto důvodu program očekává dodatečnou informaci o horním odhadu.

Složitost tohoto algoritmu při řešení hrubou silou je (doplnit).
Pro vstupy o malých rozměrech šachovnice je možné problém tímto způsobem řešit.
Časová složit ale roste exponencionálně s velikostí vstupu, pro velké $n$ je tak možné, že program nenalezne řešení v konečném čase.

Pro vstupy o velikosti 10 šachovnicových polí už doba běhu může přesáhnout jednotky minut.
Je-li vstup nepatrně větší (např. 15 polí), program už nedoběhne v přijatelné době.

Tento problém je tedy vhodný na využití paralelního výpočtu, jehož zkoumáním se práce zabývá.

\section{Popis sekvenčního algoritmu}

Sekvenční algoritmus je implementován pomocí prohledávání do hloubky (\textit{DFS}) s využitím ořezávání hranic.
Ořezáváním hranic při průchodu statového prostoru se myslí vynechání takových stavů, které už nemohou aktuální řešení vylepšit.

Algoritmus začíná na pozici královny, odkud další řešení vyhledává pohybem vždy o právě jedno pole do všech směrů.
Každý z těchto pohybů je novým stavem, odkud se královna opět vydává všemi směry.
Tento postup je tedy implementován pomocí rekurze.

Vstupem rekurzivní funkce je aktuální délka cesty (hloubka řešení), počet sebraných pěšců a cestu, kterou se královna na danné pole dostala.
Rekurzivní je volání je ukončeno, v takovém kroku, kdy královna zatím nesebrala všechny pěšce ale dalším krokem by přesáhla horní odhad.
Taková cesta pak není řešením.

Další optimalizací tohoto algoritmu je udržování doposud nalezeného minima.
V rekurzivní funkci je následně možné odhalit, že cesta nebude řešením, pokud součet aktuální délky cesty a počtu zbývajících pěšců přesahuje aktuální minimum.
Tyto optimalizace jsem v úvodu představil jako metodu ořezávání větví.

V implementaci není využito žádných heuristik, z tohoto důvodu je pravděpodobně oproti referenčnímu řešení u větších vstupních dat zhruba dvakrát pomaleší.

Měření (doplnit).

\section{Popis paralelního algoritmu a jeho implementace\\v OpenMP}

Paralelní algoritmus má velmi podobnou implementaci jako sekvenční řešení.
Tento problém je možné paralelizovat dvěma způsoby -- úkolově a datově.
Popisem obou způsobů se zabývám níže.

Při výpočtu v prostředí \textit{OpenMP} je spuštěn jeden proces, který pracuje na více vláknech.
Vlákna mají sdílenou paměť, do které zapisují průběžně nejlepší nalezené řešení.
Pro zamezenení časově závislých chyb se využívá kritických sekcí -- zde vlákna k zápisu do sdílené paměti využívají sekvenční přístup.

\subsection{Úkolově orientovaný paralelismus}

Úkolově orientovaný paralelismus je implementovaný pomocí direktiv \texttt{omp parallel} a \texttt{omp task}.
První z direktiv informuje \textit{OpenMP} o začátku paraleního výpočtu.
Druhá určuje, jaký konkrétně výpočet se bude paralelizovat.
Díky implicitní bariéře paralelizovaného bloku (první direktiva) máme jistotu, že výstup algoritmus vrátí až ve chvíli, kdy všechny paralelní výpočty doběhly.

Jednotlivými úkoly jsou pak kroky královnou o jedno pole do strany.
Oproti sekvenčnímu řešení přibyly tedy pouze direktivy.

\subsection{Datově orientovaný paralelismus}

Datově orientovaný paralelismus už se od sekvenčního řešení liší více.
Je implementováno frontou úkolů, které je potřeba vypočítat.
Jednotlivými úkoly jsou počáteční stavy, které do fronty zařadilo před paralelním výpočtem hlavní vláknou.

Toto vlákno napočítá sekvenčním průchodem dostatečný počet výchozích stavů, aby zátěž rovnoměrně pokryla počet dostupných vláken.
Po naplnění fronty se přechází k paralelnímu zpracování.

Prostředí \textit{OpenMP} nabízí k tomuto výpočtu direktivu \texttt{omp parallel for}.
Ta umožňuje paralelizovat průchod \texttt{for} smyček.
Jednotlivá vlákna následně sekvenčně projdou všechny dostupné stavy z výchozí konfigurace.
Průběžně nejlepší řešení se ukládá do sdílené paměti, ke které vlákna přistupují sekvenčně.

Direktiva obsahuje implicitní bariéru, obdobně jako u úkolového paralelismu tedy program vrátí řešení až ve chvíli, kdy skončí poslední vlákno.

\section{Popis paralelního algoritmu a jeho implementace v MPI}

Prostředí \textit{MPI} nabízí oproti \textit{OpenMP} paralelizaci s distribuovanou pamětí, tedy pomocí více procesů.

Procesy jsou rozděleny na jeden hlavní (\textit{master}) a více pracovních (\textit{slave}).
Hlavní proces se stará o plánování práce pro ostatní vlákna a o vyhodnocování mezivýsledků.
Pracovní procesy dále využívají \textit{OpenMP} pro práci na více vláknech, konkrétně datový paralelismus popsaný výše.

Vstupním bodem algoritmu je funkce, která se spustí ve všech procesech.
Každý proces si následně zjistí své číslo (\textit{rank}) a podle toho vykonává práci jako \textit{master} (pokud \textit{rank} je roven 0) nebo \textit{slave}.

Hlavní vlákno si nejprve vytvoří frontu výchozích stavů, které bude zadávat pracovním vláknům.
Po vytvoření dostatečně velké fronty je každém dalšímu procesu zaslán jeden počáteční stav.

Dokud fronta není prázdná, hlavní proces čeká na přijetí výsledku.
Po přijetí zpracuje výsledek a zadá nový počáteční stav vláknu, které vrátilo výsledek.

Jakmile je fronta prázdná, hlavní proces přejde začne čekat na výsledky ostatních vláken.
Po přijetí posledního výsledku odešle informaci o ukončení výpočtu a vrátí výsledek.
Ostatní vlákna se ukončí po přijetí zprávy o ukončení výpočtu.

\subsection{Práce s distribuovanou pamětí}

Protože se v \textit{MPI} vytváří více procesů, není možné využít pro mezivýpočty sdílenou paměť.
Procesy mezi sebou tedy komunikují pomocí zasílání zpráv.
Každá zpráva má mimo jiné odesílatele, příjemce, obsah a typ (\textit{tag}).

V obsahu zprávy je nutné využít jednoduchých datových typů (např. \texttt{int}, \texttt{char} atp.) nebo jejich kompozice.
Nelze ale posílat složité datové struktury skládající se z dalších datových struktu (např. \texttt{vector}, \texttt{queue}, \dots).
Takové objekty je potřeba \textit{převést} na kompozici primitivních typů a odeslat je v této formě.
Tomuto postupu se říká \textit{serializace}.

\subsection{Serializace}

Pro zasílání komplexních datových struktur jsem využil staticky naalokované pole čísel (\textit{int*}).
Při serializaci se veškeré informace převedou na čísla podle předem daného postupu.
Vlákno, které objekt příjme provede opačný postup a sestaví si komplexní objekt.
Dále může pracovat stejným způsobem, jako při implementaci s \textit{OpenMP}.

\section{Naměřené výsledky a vyhodnocení}

\section{Závěr}

\end{document}